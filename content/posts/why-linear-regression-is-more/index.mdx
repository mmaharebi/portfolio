---
title: "Why Linear Regression Is More Profound Than You Think: A Journey Through Estimation Theory"
date: "2025-11-27"
description: "An exploration of how geometry, probability, and sequential estimation converge on the elegant solution to linear regression, revealing deep connections across machine learning."
tags:
  [
    "linear algebra",
    "estimation theory",
    "regression",
    "pseudoinverse",
    "kalman filter",
    "machine learning",
  ]
author: "Mohammadmahdi Maharebi"
---

Linear regression is often the first algorithm encountered in machine learning, prized for its simplicity and interpretability. But this apparent simplicity is deceptive. Beneath the surface, linear regression serves as a gateway to a rich, interconnected world of geometric, probabilistic, and dynamic systems concepts.

This post revisits the classic least-squares problem to uncover the deeper mathematical structures at its core. We will journey through four distinct but convergent perspectives:

1.  **Geometric Intuition**: Viewing regression as a projection in vector spaces.
2.  **Probabilistic Rigor**: Framing it as a maximum likelihood estimation problem and invoking the Gauss-Markov theorem.
3.  **Bayesian Inference**: Understanding regularization as the incorporation of prior beliefs.
4.  **Sequential Estimation**: Evolving the batch solution into a recursive one suitable for real-time systems, leading us to the Kalman Filter and its surprising links to reinforcement learning.

## 1. The Geometric View: Regression as Projection

The most elegant and fundamental perspective on linear regression is geometric. We are given a set of input vectors $\boldsymbol{\phi}(x_i) \in \mathbb{R}^d$ and corresponding scalar outputs $y_i$. Our goal is to find a linear combination of the features that best approximates the outputs.

Let's assemble our data:

- The **design matrix** $X \in \mathbb{R}^{N \times d}$, whose rows are the feature vectors $\boldsymbol{\phi}(x_i)^\top$.
- The **target vector** $\mathbf{y} \in \mathbb{R}^N$.
- The **parameter vector** $\mathbf{w} \in \mathbb{R}^d$.

Our model predicts $\hat{\mathbf{y}} = X\mathbf{w}$. The vector $\hat{\mathbf{y}}$ is, by definition, a linear combination of the columns of $X$. This means $\hat{\mathbf{y}}$ must lie in the **column space** of $X$, denoted $\mathcal{C}(X)$.

The ordinary least squares (OLS) problem seeks to find the parameter vector $\mathbf{w}^*$ that minimizes the squared Euclidean distance between the prediction and the true targets:

$$
\mathbf{w}^* = \arg\min_{\mathbf{w}} \|\mathbf{y} - X\mathbf{w}\|^2
$$

Geometrically, this is equivalent to finding the vector $\hat{\mathbf{y}} = X\mathbf{w}^*$ in the column space $\mathcal{C}(X)$ that is closest to $\mathbf{y}$. This vector is the **orthogonal projection** of $\mathbf{y}$ onto $\mathcal{C}(X)$.

This geometric condition implies that the residual vector, $\mathbf{e} = \mathbf{y} - X\mathbf{w}^*$, must be orthogonal to every vector in $\mathcal{C}(X)$. This can be stated succinctly as:

$$
X^\top (\mathbf{y} - X\mathbf{w}^*) = \mathbf{0}
$$

Rearranging this gives the celebrated **normal equations**:

$$
X^\top X \mathbf{w}^* = X^\top \mathbf{y}
$$

If the columns of $X$ are linearly independent, the Gram matrix $X^\top X$ is invertible, and we obtain the unique solution:

$$
\boxed{\mathbf{w}^* = (X^\top X)^{-1} X^\top \mathbf{y}}
$$

The matrix $(X^\top X)^{-1} X^\top$ is the **Moore-Penrose pseudoinverse**, often denoted $X^+$. The solution $\mathbf{w}^* = X^+ \mathbf{y}$ exists even when $X^\top X$ is not invertible, providing the minimum-norm solution among all possible minimizers.

## 2. The Probabilistic View: Maximum Likelihood & The Gauss-Markov Theorem

The geometric view is deterministic. To introduce statistical properties, we model the data-generating process. A standard assumption is that the outputs are generated by a linear model with additive, zero-mean Gaussian noise:

$$y_i = \mathbf{w}^\top \boldsymbol{\phi}(x_i) + v_i, \qquad v_i \sim \mathcal{N}(0, \sigma^2)$$

In vector form, this is $\mathbf{y} = X\mathbf{w} + \mathbf{v}$, where $\mathbf{v} \sim \mathcal{N}(\mathbf{0}, \sigma^2 I)$.

From this, we can ask: what parameter vector $\mathbf{w}$ most likely generated the observed data $\mathbf{y}$? The **likelihood function** gives us the probability density of the data given the parameters:

$$
p(\mathbf{y} \mid \mathbf{w}) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mathbf{w}^\top \boldsymbol{\phi}(x_i))^2}{2\sigma^2}\right) \propto \exp\left(-\frac{1}{2\sigma^2} \|\mathbf{y} - X\mathbf{w}\|^2\right)
$$

To find the **Maximum Likelihood Estimator (MLE)**, we typically maximize the log-likelihood, which is equivalent and mathematically simpler:

$$
\log p(\mathbf{y} \mid \mathbf{w}) = C - \frac{1}{2\sigma^2} \|\mathbf{y} - X\mathbf{w}\|^2
$$

Maximizing this expression is equivalent to minimizing $\|\mathbf{y} - X\mathbf{w}\|^2$. The solution is identical to the OLS estimator:

$$
\boxed{\hat{\mathbf{w}}_{\mathrm{ML}} = (X^\top X)^{-1} X^\top \mathbf{y}}
$$

This reveals that OLS is not merely a convenient heuristic; it is the statistically optimal estimator under the Gaussian noise assumption. This allows us to analyze its properties:

- **Unbiasedness:** The estimator is correct on average: $\mathbb{E}[\hat{\mathbf{w}}_{\mathrm{ML}}] = \mathbf{w}$.
- **Covariance:** The uncertainty in our estimate is $\mathrm{Cov}(\hat{\mathbf{w}}_{\mathrm{ML}}) = \sigma^2 (X^\top X)^{-1}$.
- **Efficiency:** The estimator achieves the **Cramér-Rao Lower Bound**, meaning it is the most precise unbiased estimator possible.

Crucially, the **Gauss-Markov Theorem** provides a weaker but more general guarantee: even if the noise is not Gaussian, as long as it is uncorrelated and has zero mean and constant variance (homoscedastic), the OLS estimator is the **Best Linear Unbiased Estimator (BLUE)**. It has the minimum variance among all linear unbiased estimators.

## 3. The Bayesian View: From Regularization to Priors

The MLE perspective assumes we know nothing about $\mathbf{w}$ beforehand. Bayesian inference allows us to incorporate **prior beliefs**. We treat $\mathbf{w}$ as a random variable with a prior distribution $p(\mathbf{w})$.

Let's assume a zero-mean Gaussian prior for $\mathbf{w}$, which encodes a belief that smaller parameter values are more likely:

$$
\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \Sigma_p)
$$

Using Bayes' rule, the posterior distribution of $\mathbf{w}$ after observing the data is:

$$
p(\mathbf{w} \mid \mathbf{y}) = \frac{p(\mathbf{y} \mid \mathbf{w}) p(\mathbf{w})}{p(\mathbf{y})} \propto p(\mathbf{y} \mid \mathbf{w}) p(\mathbf{w})
$$

The **Maximum A Posteriori (MAP)** estimate maximizes this posterior probability. Taking the negative log of the posterior gives:

$$
-\log p(\mathbf{w} \mid \mathbf{y}) \propto \frac{1}{2\sigma^2} \|\mathbf{y} - X\mathbf{w}\|^2 + \frac{1}{2} \mathbf{w}^\top \Sigma_p^{-1} \mathbf{w}
$$

If we assume a simple spherical prior $\Sigma_p = \tau^2 I$, the MAP estimate becomes the minimizer of:

$$
L(\mathbf{w}) = \|\mathbf{y} - X\mathbf{w}\|^2 + \frac{\sigma^2}{\tau^2} \|\mathbf{w}\|^2
$$

The solution to this is:

$$
\boxed{\hat{\mathbf{w}}_{\mathrm{MAP}} = (X^\top X + \lambda I)^{-1} X^\top \mathbf{y}} \quad \text{where} \quad \lambda = \frac{\sigma^2}{\tau^2}
$$

This is exactly **Ridge Regression**. The regularization term, often seen as an ad-hoc penalty to prevent overfitting, is now revealed to be the consequence of a Gaussian prior on the parameters.

Other priors lead to different regularizers. For instance, a **Laplacian prior** $p(\mathbf{w}) \propto \exp(-\alpha \|\mathbf{w}\|_1)$ results in an $L_1$ penalty, leading to **LASSO** regression, which encourages sparse solutions.

## 4. Sequential Estimation: RLS and the Kalman Filter

So far, our solutions are "batch" — they require all data at once. In many real-world systems (robotics, finance, online services), data arrives sequentially. We need an efficient way to update our estimate.

The **Recursive Least Squares (RLS)** algorithm does exactly this. It provides an update rule for $\mathbf{w}_k$ given a new data point $$(\boldsymbol{\phi}_k, y_k)$$:

$$
\mathbf{w}_k = \mathbf{w}_{k-1} + \mathbf{g}_k (y_k - \boldsymbol{\phi}_k^\top \mathbf{w}_{k-1})
$$

$$
\mathbf{g}_k = P_k \boldsymbol{\phi}_k = \frac{P_{k-1}\boldsymbol{\phi}_k}{1 + \boldsymbol{\phi}_k^\top P_{k-1}\boldsymbol{\phi}_k}
$$

$$
P_k = (I - \mathbf{g}_k \boldsymbol{\phi}_k^\top) P_{k-1}
$$

where $P_k = (X_k^\top X_k)^{-1}$ is the inverse covariance matrix, updated recursively.

This structure is a specific instance of a more general and powerful tool: the **Kalman Filter**. The Kalman filter is the optimal recursive estimator for linear dynamical systems with Gaussian noise. Consider a system where the state $\mathbf{x}_k$ evolves over time:

$$
\mathbf{x}_k = A \mathbf{x}_{k-1} + \mathbf{w}_k \quad (\text{Process model}, \mathbf{w}_k \sim \mathcal{N}(0, Q))
$$

$$
\mathbf{y}_k = H_k \mathbf{x}_k + \mathbf{v}_k \quad (\text{Measurement model}, \mathbf{v}_k \sim \mathcal{N}(0, R))
$$

The Kalman filter provides the optimal estimate of $\mathbf{x}_k$ through a two-step predict-correct cycle. If we consider our regression parameter $\mathbf{w}$ to be a static state ($\mathbf{x}_k = \mathbf{w}$, $A=I$, $Q=0$) observed through measurements $(\mathbf{y}_k, H_k=\boldsymbol{\phi}_k^\top)$, the Kalman filter equations reduce to the RLS algorithm.

The filter's core update is:

$$
\hat{\mathbf{x}}_{k|k} = \hat{\mathbf{x}}_{k|k-1} + K_k (\mathbf{y}_k - H_k \hat{\mathbf{x}}_{k|k-1})
$$

The term $(\mathbf{y}_k - H_k \hat{\mathbf{x}}_{k|k-1})$ is the **innovation** or **measurement residual** — the new information not predicted by the model. The **Kalman gain** $K_k$ determines how much to trust this innovation.

## 5. The Bridge to Reinforcement Learning

This recursive, error-driven update structure forms a conceptual bridge to modern reinforcement learning. Consider the **temporal-difference (TD) learning** update for a value function $Q(s, a)$:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left(r + \gamma \max_{a'} Q(s', a') - Q(s, a)\right)
$$

The structure is strikingly similar to the Kalman filter update:

$$
	\text{New Estimate} \leftarrow \text{Old Estimate} + \text{Gain} \times (\text{Prediction Error})
$$

The TD error acts as the innovation signal, correcting the value estimate based on new experience. This parallel is not merely an analogy. When using linear function approximation for value functions, many RL algorithms are, in fact, forms of recursive least-squares estimation (e.g., LSTD, TD($\lambda$)). Both fields are fundamentally about using noisy, sequential data to recursively estimate hidden quantities of interest, whether they are physical states or optimal values.

## 6. Conclusion

Linear regression is far more than a simple curve-fitting tool. It is a microcosm of the core principles of estimation and learning. By viewing it through different lenses, we uncover a profound web of connections:

- **Geometry** reveals the solution as an optimal projection in a vector space.
- **Probability** justifies it as the most likely estimator under standard noise models and the best linear unbiased estimator more generally.
- **Bayesian Inference** recasts regularization as a principled way to encode prior knowledge.
- **State-space models** transform it into a powerful recursive estimator that operates in real-time and provides the blueprint for algorithms in signal processing, control theory, and even reinforcement learning.

This journey reveals a unifying theme: learning is a process of updating beliefs in the face of new data. And linear regression, in its elegance and depth, is our first and most fundamental guide to this landscape.
